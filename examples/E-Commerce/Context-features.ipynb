{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36495d14-2856-4d70-af53-84c8eacc9c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Text\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "from recommender import dataset\n",
    "from recommender.model import ECommerceModel\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bf79d23-5b91-4699-a819-ed5a2ef5d948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-06 09:14:58.166846: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-06-06 09:14:58.166874: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ubuntudavid-ThinkPad-P15-Gen-1\n",
      "2022-06-06 09:14:58.166881: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ubuntudavid-ThinkPad-P15-Gen-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-06 09:14:58.166933: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.129.6\n",
      "2022-06-06 09:14:58.166951: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.129.6\n",
      "2022-06-06 09:14:58.166956: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.129.6\n"
     ]
    }
   ],
   "source": [
    "devices = tf.config.list_physical_devices('GPU')\n",
    "devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05797f6d-c8cb-486a-8b51-8c55f465bbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (18067, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438319</th>\n",
       "      <td>574311</td>\n",
       "      <td>23382</td>\n",
       "      <td>BOX OF 6 CHRISTMAS CAKE DECORATIONS</td>\n",
       "      <td>6</td>\n",
       "      <td>2011-11-03 16:56:00</td>\n",
       "      <td>3.75</td>\n",
       "      <td>15640</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1.320339e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28901</th>\n",
       "      <td>538662</td>\n",
       "      <td>79321</td>\n",
       "      <td>CHILLI LIGHTS</td>\n",
       "      <td>24</td>\n",
       "      <td>2010-12-13 15:44:00</td>\n",
       "      <td>4.25</td>\n",
       "      <td>15159</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1.292255e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466055</th>\n",
       "      <td>576301</td>\n",
       "      <td>22470</td>\n",
       "      <td>HEART OF WICKER LARGE</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-11-14 14:40:00</td>\n",
       "      <td>2.95</td>\n",
       "      <td>14667</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1.321282e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194702</th>\n",
       "      <td>553663</td>\n",
       "      <td>21080</td>\n",
       "      <td>SET/20 RED RETROSPOT PAPER NAPKINS</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-05-18 12:13:00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>14527</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1.305721e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55819</th>\n",
       "      <td>540999</td>\n",
       "      <td>21633</td>\n",
       "      <td>SUNFLOWER DECORATIVE PARASOL</td>\n",
       "      <td>30</td>\n",
       "      <td>2011-01-13 10:08:00</td>\n",
       "      <td>3.95</td>\n",
       "      <td>13694</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1.294913e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       InvoiceNo StockCode                          Description  Quantity  \\\n",
       "438319    574311     23382  BOX OF 6 CHRISTMAS CAKE DECORATIONS         6   \n",
       "28901     538662     79321                        CHILLI LIGHTS        24   \n",
       "466055    576301     22470                HEART OF WICKER LARGE         4   \n",
       "194702    553663     21080  SET/20 RED RETROSPOT PAPER NAPKINS          3   \n",
       "55819     540999     21633         SUNFLOWER DECORATIVE PARASOL        30   \n",
       "\n",
       "               InvoiceDate  UnitPrice CustomerID         Country     timestamp  \n",
       "438319 2011-11-03 16:56:00       3.75      15640  United Kingdom  1.320339e+09  \n",
       "28901  2010-12-13 15:44:00       4.25      15159  United Kingdom  1.292255e+09  \n",
       "466055 2011-11-14 14:40:00       2.95      14667  United Kingdom  1.321282e+09  \n",
       "194702 2011-05-18 12:13:00       0.85      14527  United Kingdom  1.305721e+09  \n",
       "55819  2011-01-13 10:08:00       3.95      13694  United Kingdom  1.294913e+09  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    'data/Online-Retail.csv',\n",
    "    dtype={'CustomerID': str, 'StockCode': str},\n",
    "    parse_dates=['InvoiceDate'],\n",
    ")\n",
    "data = dataset.preprocess_data(data)\n",
    "\n",
    "# item_id -> description\n",
    "item_to_description = dict(data[['StockCode', 'Description']].values)\n",
    "\n",
    "print(f'Data shape {data.shape}')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73da5de7-c534-44f8-b1d1-b5bb841bcf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-06 09:15:00.841542: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "purchases = dataset.create_tf_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f484d1f5-fe6e-421e-bc35-f62cb2fa28c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = purchases.map(lambda x: {\n",
    "    'user_id': x['user_id'], \n",
    "    'item_id': x['item_id'],\n",
    "    'timestamp': x['timestamp']\n",
    "})\n",
    "users = purchases.map(lambda x: x['user_id'])\n",
    "items = purchases.map(lambda x: x['item_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5429b811-a959-4f5d-ac04-d5f2c4f2c55b",
   "metadata": {},
   "source": [
    "Now we prepare the timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e41f9b25-9d6c-47af-a6e9-77835bc84d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.concatenate(list(purchases.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
    "\n",
    "max_timestamp = timestamps.max()\n",
    "min_timestamp = timestamps.min()\n",
    "\n",
    "timestamp_buckets = np.linspace(\n",
    "    min_timestamp, max_timestamp, num=1000,\n",
    ")\n",
    "\n",
    "unique_items_titles = np.unique(np.concatenate(list(items.batch(1000))))\n",
    "unique_user_ids = np.unique(np.concatenate(list(interactions.batch(1_000).map(\n",
    "    lambda x: x[\"user_id\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8364fd-b196-4968-80e5-faf8347c776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserModel(tf.keras.Model):\n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "\n",
    "        self._use_timestamps = use_timestamps\n",
    "\n",
    "        self.user_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_user_ids, mask_token=None\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if use_timestamps:\n",
    "            self.timestamp_embedding = tf.keras.Sequential(\n",
    "                [\n",
    "                    tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
    "                    tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
    "                ]\n",
    "            )\n",
    "            self.normalized_timestamp = tf.keras.layers.Normalization(axis=None)\n",
    "\n",
    "            self.normalized_timestamp.adapt(timestamps)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if not self._use_timestamps:\n",
    "            return self.user_embedding(inputs[\"user_id\"])\n",
    "\n",
    "        return tf.concat(\n",
    "            [\n",
    "                self.user_embedding(inputs[\"user_id\"]),\n",
    "                self.timestamp_embedding(inputs[\"timestamp\"]),\n",
    "                tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1)),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4ad364-5497-4f21-81e8-7361a1a5a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        max_tokens = 10_000\n",
    "\n",
    "        self.title_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.StringLookup(\n",
    "                    vocabulary=unique_items_titles, mask_token=None\n",
    "                ),\n",
    "                tf.keras.layers.Embedding(len(unique_items_titles) + 1, 32),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.title_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_tokens)\n",
    "\n",
    "        self.title_text_embedding = tf.keras.Sequential(\n",
    "            [\n",
    "                self.title_vectorizer,\n",
    "                tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
    "                tf.keras.layers.GlobalAveragePooling1D(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.title_vectorizer.adapt(items)\n",
    "\n",
    "    def call(self, titles):\n",
    "        return tf.concat(\n",
    "            [\n",
    "                self.title_embedding(titles),\n",
    "                self.title_text_embedding(titles),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2759c1bb-8a61-415c-9471-4739b740678f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECommerceModel(tfrs.models.Model):\n",
    "    def __init__(self, use_timestamps):\n",
    "        super().__init__()\n",
    "        self.query_model = tf.keras.Sequential(\n",
    "            [UserModel(use_timestamps), tf.keras.layers.Dense(32)]\n",
    "        )\n",
    "        self.candidate_model = tf.keras.Sequential(\n",
    "            [ItemModel(), tf.keras.layers.Dense(32)]\n",
    "        )\n",
    "        self.task = tfrs.tasks.Retrieval(\n",
    "            metrics=tfrs.metrics.FactorizedTopK(\n",
    "                candidates=items.batch(128).map(self.candidate_model),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, features, training=False):\n",
    "        # We only pass the user id and timestamp features into the query model. This\n",
    "        # is to ensure that the training inputs would have the same keys as the\n",
    "        # query inputs. Otherwise the discrepancy in input structure would cause an\n",
    "        # error when loading the query model after saving it.\n",
    "        query_embeddings = self.query_model(\n",
    "            {\n",
    "                \"user_id\": features[\"user_id\"],\n",
    "                \"timestamp\": features[\"timestamp\"],\n",
    "            }\n",
    "        )\n",
    "        movie_embeddings = self.candidate_model(features[\"item_id\"])\n",
    "\n",
    "        return self.task(query_embeddings, movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "402ebc91-10dc-4e83-ac53-2c93347f2f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18067"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1e5ce7e-2ae5-4c2d-b3e8-8926f980c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = interactions.shuffle(18_067, seed=42, reshuffle_each_iteration=False)\n",
    "\n",
    "train = shuffled.take(17_067)\n",
    "test = shuffled.skip(17_067).take(1_000)\n",
    "\n",
    "cached_train = train.shuffle(17_067).batch(2048)\n",
    "cached_test = test.batch(512).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c859c8-cab1-40b5-8226-961f29fdf3a7",
   "metadata": {},
   "source": [
    "## Baseline: no timestamp features\n",
    "We're ready to try out our first model: let's start with not using timestamp features to establish our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ca807-13b8-4fd6-abe9-cae6e4b9bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float64>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float64>}. Consider rewriting this model with the Functional API.\n",
      "2/5 [===========>..................] - ETA: 2s - factorized_top_k/top_1_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_5_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_10_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_50_categorical_accuracy: 0.0000e+00 - factorized_top_k/top_100_categorical_accuracy: 0.0000e+00 - loss: 34068.6133 - regularization_loss: 0.0000e+00 - total_loss: 34068.6133"
     ]
    }
   ],
   "source": [
    "model = ECommerceModel(use_timestamps=False)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", update_freq='epoch')\n",
    "\n",
    "model.fit(\n",
    "    x=train.batch(4096),\n",
    "    validation_data=test.batch(4096),\n",
    "    epochs=5,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20d5f7d1-51bf-43e7-9c54-fc80f9e15095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 8s 826ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0485 - factorized_top_k/top_5_categorical_accuracy: 0.0920 - factorized_top_k/top_10_categorical_accuracy: 0.1245 - factorized_top_k/top_50_categorical_accuracy: 0.2460 - factorized_top_k/top_100_categorical_accuracy: 0.3288 - loss: 9994.1339 - regularization_loss: 0.0000e+00 - total_loss: 9994.1339\n",
      "2/2 [==============================] - 1s 277ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0460 - factorized_top_k/top_5_categorical_accuracy: 0.0950 - factorized_top_k/top_10_categorical_accuracy: 0.1300 - factorized_top_k/top_50_categorical_accuracy: 0.2450 - factorized_top_k/top_100_categorical_accuracy: 0.3150 - loss: 2139.6772 - regularization_loss: 0.0000e+00 - total_loss: 2139.6772\n",
      "Top-100 accuracy (train): 0.33.\n",
      "Top-100 accuracy (test): 0.31.\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = model.evaluate(\n",
    "    cached_train, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "test_accuracy = model.evaluate(\n",
    "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "\n",
    "print(f\"Top-100 accuracy (train): {train_accuracy:.2f}.\")\n",
    "print(f\"Top-100 accuracy (test): {test_accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4118d2a2-491b-46d5-abbc-e4cd1d03e6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': b'15640', 'item_id': b'23382', 'timestamp': 1320339360.0}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "11811abc-7a84-4887-af42-702907070c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec={'user_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'item_id': TensorSpec(shape=(), dtype=tf.string, name=None), 'timestamp': TensorSpec(shape=(), dtype=tf.float64, name=None)}>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def329a-4396-4eba-9930-65ba6b5a542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = interactions.take(1).as_numpy_iterator()\n",
    "\n",
    "# Use brute-force search to set up retrieval using the trained representations.\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
    "index.index_from_dataset(\n",
    "    unique_items.batch(128).map(lambda title: (title, model.candidate_model(title)))\n",
    ")\n",
    "\n",
    "scores, items = index(np.array(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a432661d-7238-42f0-a357-9e8253ab062c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History :\n",
      "        StockCode                        Description\n",
      "407540     23556      LANDMARK FRAME COVENT GARDEN \n",
      "448490     22712                   CARD DOLLY GIRL \n",
      "467556     82484  WOOD BLACK BOARD ANT WHITE FINISH\n",
      "478575     23389             SPACEBOY MINI BACKPACK\n",
      "518061     22294         HEART FILIGREE DOVE  SMALL\n",
      "518861     23084                 RABBIT NIGHT LIGHT\n",
      "526603     23371    SET 36 COLOUR PENCILS SPACEBOY \n",
      "526610     23209           LUNCH BAG VINTAGE DOILY \n",
      "526619     22835    HOT WATER BOTTLE I AM SO POORLY\n",
      "526658     22457    NATURAL SLATE HEART CHALKBOARD \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"user_model_7\" (type UserModel).\n\nOnly integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'user_id'\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(1,), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48286/2896102342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get some recommendations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrecommendations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrecommendations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Item_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-recommenders/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-recommenders/lib/python3.7/site-packages/tensorflow_recommenders/layers/factorized_top_k.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, queries, k)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m       \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48286/2746766037.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_timestamps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     return tf.concat([\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer \"user_model_7\" (type UserModel).\n\nOnly integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'user_id'\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(1,), dtype=string)"
     ]
    }
   ],
   "source": [
    "user_id = '13089'\n",
    "\n",
    "past_purchases = data[data['CustomerID'] == user_id].sort_values(by='InvoiceDate')\n",
    "print(f\"History :\\n {past_purchases[['StockCode', 'Description']].tail(10)}\\n\")\n",
    "\n",
    "# Use brute-force search to set up retrieval using the trained representations.\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
    "index.index_from_dataset(\n",
    "    unique_items.batch(128).map(lambda title: (title, model.candidate_model(title)))\n",
    ")\n",
    "\n",
    "# Get some recommendations.\n",
    "scores, items = index(np.array([user_id]))\n",
    "recommendations = pd.DataFrame()\n",
    "recommendations['Item_id'] = items.numpy().flatten()\n",
    "recommendations['Description'] = recommendations['Item_id'].apply(lambda x: x.decode('utf-8')).map(item_to_description)\n",
    "recommendations['Scores'] = scores.numpy().flatten()\n",
    "\n",
    "print(f'Recommendations\\n {recommendations}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d848c36-f0aa-49e5-b4b3-7df171ec1467",
   "metadata": {},
   "source": [
    "## Capturing time dynamics with time features\n",
    "Do the result change if we add time features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a11c83a-c05d-4497-a66b-76d60bf6a8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float64>}. Consider rewriting this model with the Functional API.\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float64>}. Consider rewriting this model with the Functional API.\n",
      "4/4 [==============================] - ETA: 0s - factorized_top_k/top_1_categorical_accuracy: 0.0670 - factorized_top_k/top_5_categorical_accuracy: 0.0670 - factorized_top_k/top_10_categorical_accuracy: 0.0673 - factorized_top_k/top_50_categorical_accuracy: 0.0740 - factorized_top_k/top_100_categorical_accuracy: 0.0795 - loss: 35000.3262 - regularization_loss: 0.0000e+00 - total_loss: 35000.3262                  WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs={'user_id': <tf.Tensor 'IteratorGetNext:2' shape=(None,) dtype=string>, 'timestamp': <tf.Tensor 'IteratorGetNext:1' shape=(None,) dtype=float64>}. Consider rewriting this model with the Functional API.\n",
      "4/4 [==============================] - 9s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0670 - factorized_top_k/top_5_categorical_accuracy: 0.0670 - factorized_top_k/top_10_categorical_accuracy: 0.0673 - factorized_top_k/top_50_categorical_accuracy: 0.0740 - factorized_top_k/top_100_categorical_accuracy: 0.0795 - loss: 34849.3063 - regularization_loss: 0.0000e+00 - total_loss: 34849.3063 - val_factorized_top_k/top_1_categorical_accuracy: 0.0011 - val_factorized_top_k/top_5_categorical_accuracy: 0.0011 - val_factorized_top_k/top_10_categorical_accuracy: 0.0011 - val_factorized_top_k/top_50_categorical_accuracy: 0.0039 - val_factorized_top_k/top_100_categorical_accuracy: 0.0083 - val_loss: 14279.8594 - val_regularization_loss: 0.0000e+00 - val_total_loss: 14279.8594\n",
      "Epoch 2/5\n",
      "4/4 [==============================] - 8s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0131 - factorized_top_k/top_5_categorical_accuracy: 0.0137 - factorized_top_k/top_10_categorical_accuracy: 0.0149 - factorized_top_k/top_50_categorical_accuracy: 0.0274 - factorized_top_k/top_100_categorical_accuracy: 0.0371 - loss: 33869.7930 - regularization_loss: 0.0000e+00 - total_loss: 33869.7930 - val_factorized_top_k/top_1_categorical_accuracy: 0.0011 - val_factorized_top_k/top_5_categorical_accuracy: 0.0017 - val_factorized_top_k/top_10_categorical_accuracy: 0.0017 - val_factorized_top_k/top_50_categorical_accuracy: 0.0039 - val_factorized_top_k/top_100_categorical_accuracy: 0.0083 - val_loss: 13542.6865 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13542.6865\n",
      "Epoch 3/5\n",
      "4/4 [==============================] - 8s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0039 - factorized_top_k/top_5_categorical_accuracy: 0.0050 - factorized_top_k/top_10_categorical_accuracy: 0.0065 - factorized_top_k/top_50_categorical_accuracy: 0.0187 - factorized_top_k/top_100_categorical_accuracy: 0.0296 - loss: 32599.0777 - regularization_loss: 0.0000e+00 - total_loss: 32599.0777 - val_factorized_top_k/top_1_categorical_accuracy: 5.5340e-04 - val_factorized_top_k/top_5_categorical_accuracy: 5.5340e-04 - val_factorized_top_k/top_10_categorical_accuracy: 5.5340e-04 - val_factorized_top_k/top_50_categorical_accuracy: 0.0061 - val_factorized_top_k/top_100_categorical_accuracy: 0.0094 - val_loss: 13651.9424 - val_regularization_loss: 0.0000e+00 - val_total_loss: 13651.9424\n",
      "Epoch 4/5\n",
      "4/4 [==============================] - 8s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0028 - factorized_top_k/top_5_categorical_accuracy: 0.0053 - factorized_top_k/top_10_categorical_accuracy: 0.0105 - factorized_top_k/top_50_categorical_accuracy: 0.0360 - factorized_top_k/top_100_categorical_accuracy: 0.0597 - loss: 31181.5582 - regularization_loss: 0.0000e+00 - total_loss: 31181.5582 - val_factorized_top_k/top_1_categorical_accuracy: 5.5340e-04 - val_factorized_top_k/top_5_categorical_accuracy: 0.0011 - val_factorized_top_k/top_10_categorical_accuracy: 0.0017 - val_factorized_top_k/top_50_categorical_accuracy: 0.0072 - val_factorized_top_k/top_100_categorical_accuracy: 0.0105 - val_loss: 14156.9863 - val_regularization_loss: 0.0000e+00 - val_total_loss: 14156.9863\n",
      "Epoch 5/5\n",
      "4/4 [==============================] - 8s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0039 - factorized_top_k/top_5_categorical_accuracy: 0.0146 - factorized_top_k/top_10_categorical_accuracy: 0.0274 - factorized_top_k/top_50_categorical_accuracy: 0.0802 - factorized_top_k/top_100_categorical_accuracy: 0.1173 - loss: 29340.1488 - regularization_loss: 0.0000e+00 - total_loss: 29340.1488 - val_factorized_top_k/top_1_categorical_accuracy: 0.0011 - val_factorized_top_k/top_5_categorical_accuracy: 0.0022 - val_factorized_top_k/top_10_categorical_accuracy: 0.0022 - val_factorized_top_k/top_50_categorical_accuracy: 0.0072 - val_factorized_top_k/top_100_categorical_accuracy: 0.0116 - val_loss: 14993.8555 - val_regularization_loss: 0.0000e+00 - val_total_loss: 14993.8555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f72787f71d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ECommerceModel(use_timestamps=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\", update_freq='epoch')\n",
    "\n",
    "model.fit(\n",
    "    x=train_dataset.batch(4096),\n",
    "    validation_data=val_dataset.batch(4096),\n",
    "    epochs=5,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67e4af5b-11fc-4921-ae1c-40d1f3d06081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 8s 849ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0382 - factorized_top_k/top_5_categorical_accuracy: 0.0691 - factorized_top_k/top_10_categorical_accuracy: 0.0945 - factorized_top_k/top_50_categorical_accuracy: 0.1919 - factorized_top_k/top_100_categorical_accuracy: 0.2507 - loss: 10766.3431 - regularization_loss: 0.0000e+00 - total_loss: 10766.3431\n",
      "2/2 [==============================] - 1s 289ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0420 - factorized_top_k/top_5_categorical_accuracy: 0.0770 - factorized_top_k/top_10_categorical_accuracy: 0.1090 - factorized_top_k/top_50_categorical_accuracy: 0.1960 - factorized_top_k/top_100_categorical_accuracy: 0.2440 - loss: 2375.2346 - regularization_loss: 0.0000e+00 - total_loss: 2375.2346\n",
      "Top-100 accuracy (train): 0.25.\n",
      "Top-100 accuracy (test): 0.24.\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = model.evaluate(\n",
    "    cached_train, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "test_accuracy = model.evaluate(\n",
    "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
    "\n",
    "print(f\"Top-100 accuracy (train): {train_accuracy:.2f}.\")\n",
    "print(f\"Top-100 accuracy (test): {test_accuracy:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1cf1ad39-bf71-4002-8632-1daaa8fb2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_items = {'item_id': data['StockCode'].unique()}\n",
    "unique_items = tf.data.Dataset.from_tensor_slices(unique_items)\n",
    "\n",
    "unique_items = unique_items.map(lambda x: x['item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04d34d15-d147-4e9a-b0b7-2ff362bdab99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user_id': b'15640', 'item_id': b'23382', 'timestamp': 1320339360.0}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(interactions.take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d8d006db-93da-4bee-a13b-05d05e012994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History :\n",
      "        StockCode                        Description\n",
      "407540     23556      LANDMARK FRAME COVENT GARDEN \n",
      "448490     22712                   CARD DOLLY GIRL \n",
      "467556     82484  WOOD BLACK BOARD ANT WHITE FINISH\n",
      "478575     23389             SPACEBOY MINI BACKPACK\n",
      "518061     22294         HEART FILIGREE DOVE  SMALL\n",
      "518861     23084                 RABBIT NIGHT LIGHT\n",
      "526603     23371    SET 36 COLOUR PENCILS SPACEBOY \n",
      "526610     23209           LUNCH BAG VINTAGE DOILY \n",
      "526619     22835    HOT WATER BOTTLE I AM SO POORLY\n",
      "526658     22457    NATURAL SLATE HEART CHALKBOARD \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"user_model_6\" (type UserModel).\n\nOnly integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'user_id'\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(1,), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48286/1573627140.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get some recommendations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'13089'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mrecommendations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrecommendations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Item_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-recommenders/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-recommenders/lib/python3.7/site-packages/tensorflow_recommenders/layers/factorized_top_k.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, queries, k)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m       \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48286/2746766037.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     return tf.concat([\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer \"user_model_6\" (type UserModel).\n\nOnly integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'user_id'\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(1,), dtype=string)"
     ]
    }
   ],
   "source": [
    "user_id = '13089'\n",
    "\n",
    "past_purchases = data[data['CustomerID'] == user_id].sort_values(by='InvoiceDate')\n",
    "print(f\"History :\\n {past_purchases[['StockCode', 'Description']].tail(10)}\\n\")\n",
    "\n",
    "# Use brute-force search to set up retrieval using the trained representations.\n",
    "index = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
    "index.index_from_dataset(\n",
    "    unique_items.batch(128).map(lambda title: (title, model.candidate_model(title)))\n",
    ")\n",
    "\n",
    "# Get some recommendations.\n",
    "scores, items = index(np.array([user_id]))\n",
    "recommendations = pd.DataFrame()\n",
    "recommendations['Item_id'] = items.numpy().flatten()\n",
    "recommendations['Description'] = recommendations['Item_id'].apply(lambda x: x.decode('utf-8')).map(item_to_description)\n",
    "recommendations['Scores'] = scores.numpy().flatten()\n",
    "\n",
    "print(f'Recommendations\\n {recommendations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912faea8-be5e-4894-9ca1-4a9b54af1ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
